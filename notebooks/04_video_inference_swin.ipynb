{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c8d8cdbf-d47c-406c-97f6-b089e3c2c328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- cfg: {'type': 'SampleFrames', 'clip_len': 16, 'frame_interval': 1, 'num_clips': 1, 'test_mode': True}\n",
      "--------- cfg: {'type': 'SampleFrames', 'clip_len': 16, 'frame_interval': 2, 'num_clips': 1, 'test_mode': True}\n",
      "Loads checkpoint by local backend from path: ../work_dirs/xiandemo_swin/epoch_30.pth\n",
      "labels: ['en', 'na', 'si', 'tie', 'zhuan']\n",
      "num_frames: 551\n",
      "temp_video_path: /root/project/research/action/mmaction2/demo_out/temp/tmpln8ic7vl.mp4, temp_video.name: /root/project/research/action/mmaction2/demo_out/temp/tmpln8ic7vl.mp4\n",
      "self.crop_size: (224, 224), imgs: 224, 398\n",
      "top5_label: [(3, 0.9981462359428406), (1, 0.0009266193374060094), (2, 0.0007969088619574904), (4, 0.00011363986413925886), (0, 1.6628029698040336e-05)]\n",
      "results: [(31, 'tie')]\n",
      "temp_video_path: /root/project/research/action/mmaction2/demo_out/temp/tmp0z7hkoz4.mp4, temp_video.name: /root/project/research/action/mmaction2/demo_out/temp/tmp0z7hkoz4.mp4\n",
      "self.crop_size: (224, 224), imgs: 224, 398\n",
      "top5_label: [(2, 0.9999579787254333), (3, 1.856703238445334e-05), (1, 1.526637606730219e-05), (0, 4.1273065107816365e-06), (4, 4.059754246554803e-06)]\n",
      "results: [(63, 'si')]\n",
      "temp_video_path: /root/project/research/action/mmaction2/demo_out/temp/tmpj353bgbv.mp4, temp_video.name: /root/project/research/action/mmaction2/demo_out/temp/tmpj353bgbv.mp4\n",
      "self.crop_size: (224, 224), imgs: 224, 398\n",
      "top5_label: [(3, 0.34952864050865173), (0, 0.3397228419780731), (4, 0.3097122311592102), (1, 0.0007482146611437201), (2, 0.00028807210037484765)]\n",
      "results: [(95, 'tie')]\n",
      "temp_video_path: /root/project/research/action/mmaction2/demo_out/temp/tmp_7e4eizt.mp4, temp_video.name: /root/project/research/action/mmaction2/demo_out/temp/tmp_7e4eizt.mp4\n",
      "self.crop_size: (224, 224), imgs: 224, 398\n",
      "top5_label: [(3, 0.9991227984428406), (4, 0.00042612937977537513), (0, 0.0002170474035665393), (1, 0.00020991559722460806), (2, 2.4159908207366243e-05)]\n",
      "results: [(127, 'tie')]\n",
      "temp_video_path: /root/project/research/action/mmaction2/demo_out/temp/tmplq37r8ye.mp4, temp_video.name: /root/project/research/action/mmaction2/demo_out/temp/tmplq37r8ye.mp4\n",
      "self.crop_size: (224, 224), imgs: 224, 398\n",
      "top5_label: [(4, 0.9999942779541016), (0, 5.019191121391486e-06), (3, 6.251908075682877e-07), (1, 1.165655483248429e-08), (2, 4.468686309877512e-09)]\n",
      "results: [(159, 'zhuan')]\n",
      "temp_video_path: /root/project/research/action/mmaction2/demo_out/temp/tmprjlr38dh.mp4, temp_video.name: /root/project/research/action/mmaction2/demo_out/temp/tmprjlr38dh.mp4\n",
      "self.crop_size: (224, 224), imgs: 224, 398\n",
      "top5_label: [(4, 0.616385817527771), (0, 0.3834082782268524), (3, 0.00020116686937399209), (1, 4.150924723944627e-06), (2, 6.779914087928773e-07)]\n",
      "results: [(191, 'zhuan')]\n",
      "temp_video_path: /root/project/research/action/mmaction2/demo_out/temp/tmperd2e52h.mp4, temp_video.name: /root/project/research/action/mmaction2/demo_out/temp/tmperd2e52h.mp4\n",
      "self.crop_size: (224, 224), imgs: 224, 398\n",
      "top5_label: [(0, 0.510753333568573), (1, 0.4878743588924408), (3, 0.001186397741548717), (4, 0.00016031436098273844), (2, 2.5600273147574626e-05)]\n",
      "results: [(223, 'en')]\n",
      "temp_video_path: /root/project/research/action/mmaction2/demo_out/temp/tmpjswq4vx2.mp4, temp_video.name: /root/project/research/action/mmaction2/demo_out/temp/tmpjswq4vx2.mp4\n",
      "self.crop_size: (224, 224), imgs: 224, 398\n",
      "top5_label: [(1, 0.999426543712616), (3, 0.0005390121950767934), (2, 1.6392270481446758e-05), (4, 1.1698916750901844e-05), (0, 6.348074293782702e-06)]\n",
      "results: [(255, 'na')]\n",
      "temp_video_path: /root/project/research/action/mmaction2/demo_out/temp/tmpxdw47cou.mp4, temp_video.name: /root/project/research/action/mmaction2/demo_out/temp/tmpxdw47cou.mp4\n",
      "self.crop_size: (224, 224), imgs: 224, 398\n",
      "top5_label: [(3, 0.9996662139892578), (1, 0.00025583835667930543), (4, 4.302522211219184e-05), (2, 2.9050002922303975e-05), (0, 5.864190825377591e-06)]\n",
      "results: [(287, 'tie')]\n",
      "temp_video_path: /root/project/research/action/mmaction2/demo_out/temp/tmpdy4ae1ua.mp4, temp_video.name: /root/project/research/action/mmaction2/demo_out/temp/tmpdy4ae1ua.mp4\n",
      "self.crop_size: (224, 224), imgs: 224, 398\n",
      "top5_label: [(2, 0.9977900385856628), (3, 0.0016650368925184011), (1, 0.0004991720197722316), (4, 2.5873854610836133e-05), (0, 1.985540620808024e-05)]\n",
      "results: [(319, 'si')]\n",
      "temp_video_path: /root/project/research/action/mmaction2/demo_out/temp/tmpmejkoqck.mp4, temp_video.name: /root/project/research/action/mmaction2/demo_out/temp/tmpmejkoqck.mp4\n",
      "self.crop_size: (224, 224), imgs: 224, 398\n",
      "top5_label: [(2, 0.46644657850265503), (4, 0.28686070442199707), (3, 0.21138536930084229), (0, 0.02084989845752716), (1, 0.014457511715590954)]\n",
      "results: [(351, 'si')]\n",
      "temp_video_path: /root/project/research/action/mmaction2/demo_out/temp/tmp4rxz2ehz.mp4, temp_video.name: /root/project/research/action/mmaction2/demo_out/temp/tmp4rxz2ehz.mp4\n",
      "self.crop_size: (224, 224), imgs: 224, 398\n",
      "top5_label: [(3, 0.9977335929870605), (1, 0.001581104937940836), (2, 0.0004926268593408167), (4, 0.0001592255721334368), (0, 3.339443355798721e-05)]\n",
      "results: [(383, 'tie')]\n",
      "temp_video_path: /root/project/research/action/mmaction2/demo_out/temp/tmpiy3n6dsr.mp4, temp_video.name: /root/project/research/action/mmaction2/demo_out/temp/tmpiy3n6dsr.mp4\n",
      "self.crop_size: (224, 224), imgs: 224, 398\n",
      "top5_label: [(4, 0.9998556971549988), (3, 7.391825783997774e-05), (0, 7.007946260273457e-05), (1, 2.911057492838154e-07), (2, 7.660881351512216e-08)]\n",
      "results: [(415, 'zhuan')]\n",
      "temp_video_path: /root/project/research/action/mmaction2/demo_out/temp/tmpo4w2ckp2.mp4, temp_video.name: /root/project/research/action/mmaction2/demo_out/temp/tmpo4w2ckp2.mp4\n",
      "self.crop_size: (224, 224), imgs: 224, 398\n",
      "top5_label: [(4, 0.7691056132316589), (0, 0.23088529706001282), (3, 8.835679182084277e-06), (1, 1.7636153870626003e-07), (2, 1.4599469011500332e-07)]\n",
      "results: [(447, 'zhuan')]\n",
      "temp_video_path: /root/project/research/action/mmaction2/demo_out/temp/tmpfbhn65up.mp4, temp_video.name: /root/project/research/action/mmaction2/demo_out/temp/tmpfbhn65up.mp4\n",
      "self.crop_size: (224, 224), imgs: 224, 398\n",
      "top5_label: [(1, 0.9689178466796875), (0, 0.030159372836351395), (3, 0.0008028078591451049), (4, 8.597142004873604e-05), (2, 3.4045275242533535e-05)]\n",
      "results: [(479, 'na')]\n",
      "temp_video_path: /root/project/research/action/mmaction2/demo_out/temp/tmp90mtwuuy.mp4, temp_video.name: /root/project/research/action/mmaction2/demo_out/temp/tmp90mtwuuy.mp4\n",
      "self.crop_size: (224, 224), imgs: 224, 398\n",
      "top5_label: [(3, 0.7129664421081543), (1, 0.28685441613197327), (4, 0.00013833346019964665), (2, 2.1570693206740543e-05), (0, 1.9295763195259497e-05)]\n",
      "results: [(511, 'tie')]\n",
      "temp_video_path: /root/project/research/action/mmaction2/demo_out/temp/tmpo9mvd22w.mp4, temp_video.name: /root/project/research/action/mmaction2/demo_out/temp/tmpo9mvd22w.mp4\n",
      "self.crop_size: (224, 224), imgs: 224, 398\n",
      "top5_label: [(3, 0.9930003881454468), (1, 0.0061890073120594025), (4, 0.0005059869145043194), (2, 0.0002660656173247844), (0, 3.8556172512471676e-05)]\n",
      "results: [(543, 'tie')]\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "from operator import itemgetter\n",
    "from typing import Optional, Tuple\n",
    "import tempfile\n",
    "from collections import deque\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "from mmengine import Config\n",
    "from mmaction.apis import inference_recognizer, init_recognizer\n",
    "from mmaction.visualization import ActionVisualizer\n",
    "\n",
    "# 参数直接在Notebook中定义\n",
    "config = '../configs/recognition/swin/swin-tiny-p244-w877_in1k-pre_8xb8-amp-8x1x2-30e_xiandemo-rgb.py'  # 配置文件路径\n",
    "checkpoint = '../work_dirs/xiandemo_swin/epoch_30.pth'  # 检查点文件\n",
    "video = '../data/xian_video_dataset/pre/VID_20240905_104444.mp4'  # 视频文件或帧文件路径\n",
    "# video = '../data/xian_video_dataset/val/1 (1).mp4'  # 视频文件或帧文件路径\n",
    "label_file = '../data/xian_video_dataset/label_map.txt'  # 标签文件路径\n",
    "out_filename = '../data/xian_video_dataset/output_swin_video.mp4'  # 输出文件路径\n",
    "fps = 30  # 输出视频的FPS\n",
    "font_scale = None  # 字体大小\n",
    "font_color = 'white'  # 字体颜色\n",
    "target_resolution = (256, 256)  # 目标分辨率（宽，高）\n",
    "device = 'cuda:0'  # 使用的设备\n",
    "\n",
    "clip_length = 32  # 每次推理使用的帧数\n",
    "interval = 1  # 帧间隔\n",
    "# 从配置文件中加载模型配置\n",
    "cfg = Config.fromfile(config)\n",
    "\n",
    "print(f\"-------- cfg: {cfg.test_pipeline[1]}\")\n",
    "\n",
    "cfg.test_pipeline[1].num_clips=1\n",
    "cfg.test_pipeline[1].frame_interval=2\n",
    "cfg.test_pipeline[1].clip_len=16\n",
    "\n",
    "print(f\"--------- cfg: {cfg.test_pipeline[1]}\")\n",
    "\n",
    "model = init_recognizer(cfg, checkpoint, device=device)\n",
    "\n",
    "# 获取标签\n",
    "labels = [x.strip() for x in open(label_file).readlines()]\n",
    "\n",
    "print(f\"labels: {labels}\")\n",
    "\n",
    "# 视频读取\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(video)\n",
    "num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "print(f\"num_frames: {num_frames}\")\n",
    "frame_queue = deque(maxlen=clip_length)\n",
    "\n",
    "results = []\n",
    "temp_videos = []  # 存储临时短视频片段路径\n",
    "for i in range(num_frames):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # 缓存帧用于分段推理\n",
    "    results = []\n",
    "    frame_queue.append(frame)\n",
    "    if len(frame_queue) == clip_length:\n",
    "        # 临时保存帧队列为视频文件\n",
    "        temp_dir = '/root/project/research/action/mmaction2/demo_out/temp'\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".mp4\", dir=temp_dir) as temp_video:\n",
    "            temp_video_path = os.path.join('./' , temp_video.name)\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "            print(f\"temp_video_path: {temp_video_path}, temp_video.name: {temp_video.name}\")\n",
    "            video_writer = cv2.VideoWriter(temp_video_path, fourcc, fps, (frame.shape[1], frame.shape[0]))\n",
    "\n",
    "            for buffered_frame in frame_queue:\n",
    "                video_writer.write(buffered_frame)\n",
    "            video_writer.release()\n",
    "            \n",
    "        # 推理并保存结果\n",
    "        pred_result = inference_recognizer(model, temp_video_path)\n",
    "        pred_scores = pred_result.pred_score.tolist()\n",
    "        score_sorted = sorted(enumerate(pred_scores), key=itemgetter(1), reverse=True)\n",
    "        top5_label = score_sorted[:5]\n",
    "        print(f\"top5_label: {top5_label}\")\n",
    "        top_label = labels[score_sorted[0][0]]\n",
    "        results.append((i, top_label))\n",
    "        print(f\"results: {results}\")\n",
    "\n",
    "        # 重新打开临时视频文件以添加推理结果\n",
    "        video_writer = cv2.VideoWriter(temp_video_path, fourcc, fps, (frame.shape[1], frame.shape[0]))\n",
    "        for buffered_frame in frame_queue:\n",
    "            # 在每一帧中写入识别的动作类别\n",
    "            cv2.putText(buffered_frame, f\"Action: {top_label}\", (10, 30), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "            video_writer.write(buffered_frame)\n",
    "        video_writer.release()\n",
    "        \n",
    "        temp_videos.append(temp_video_path)  # 添加至临时视频列表\n",
    "        # 清空缓冲队列，开始新的片段\n",
    "        frame_queue.clear()  \n",
    "\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1a5b6a83-72c4-4813-a1be-992127da5ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并短视频为一个长视频\n",
    "from moviepy.editor import VideoFileClip, concatenate_videoclips\n",
    "clips = [VideoFileClip(temp_path) for temp_path in temp_videos]\n",
    "# print(f\"temp_videos: {temp_videos}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ddf5108c-0fa5-47dc-8122-f952b00c2224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video ../data/xian_video_dataset/output_swin_video.mp4.\n",
      "Moviepy - Writing video ../data/xian_video_dataset/output_swin_video.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready ../data/xian_video_dataset/output_swin_video.mp4\n"
     ]
    }
   ],
   "source": [
    "final_video = concatenate_videoclips(clips)\n",
    "final_video.write_videofile(out_filename, codec=\"libx264\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fedac758-e8fd-4925-9cc7-46967bb88673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除临时文件\n",
    "for temp_path in temp_videos:\n",
    "    os.remove(temp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eb0d7e15-10ea-4d77-9cff-1be76535ee8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"../data/xian_video_dataset/output_swin_video.mp4\" controls  width=\"640\"  height=\"480\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 如果你想在Notebook中显示生成的视频，可以使用以下代码\n",
    "from IPython.display import Video\n",
    "Video('../data/xian_video_dataset/output_swin_video.mp4', width=640, height=480)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3c3f74c7-8daf-40b3-8814-60166913ae2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"/root/project/research/action/mmpose/vis_results/VID_20240905_104444.mp4\" controls  width=\"640\"  height=\"480\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 如果你想在Notebook中显示生成的视频，可以使用以下代码\n",
    "from IPython.display import Video\n",
    "Video('/root/project/research/action/mmpose/vis_results/VID_20240905_104444.mp4', width=640, height=480)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e454d9-8bd8-497f-99cc-df12a5ad6e5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
