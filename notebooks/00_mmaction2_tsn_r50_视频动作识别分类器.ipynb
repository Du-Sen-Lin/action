{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "469dad3e-d36b-4f1d-8a03-654bd0054fc8",
   "metadata": {},
   "source": [
    "# 一、环境check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06cb8931-12a2-4a83-b62d-cdb63ae5bb4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
      "Cuda compilation tools, release 11.8, V11.8.89\n",
      "Build cuda_11.8.r11.8/compiler.31833905_0\n",
      "gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\n",
      "Copyright (C) 2017 Free Software Foundation, Inc.\n",
      "This is free software; see the source for copying conditions.  There is NO\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check nvcc version\n",
    "!nvcc -V\n",
    "# Check GCC version\n",
    "!gcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b54138e1-0aaa-4e9f-ac37-13a9d2b6a1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu117 True\n"
     ]
    }
   ],
   "source": [
    "# Check Pytorch installation\n",
    "import torch, torchvision\n",
    "print(torch.__version__, torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "344b9afb-c595-415c-9fcd-ebf9ed558680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.0\n"
     ]
    }
   ],
   "source": [
    "# Check MMAction2 installation\n",
    "import mmaction\n",
    "print(mmaction.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4962bbd-04e0-48c8-9bc3-8db9acfbda24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.8\n",
      "GCC 7.5\n"
     ]
    }
   ],
   "source": [
    "# Check MMCV installation\n",
    "from mmcv.ops import get_compiling_cuda_version, get_compiler_version\n",
    "print(get_compiling_cuda_version())\n",
    "print(get_compiler_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "681bf506-0bd7-413b-a3b4-77a652232470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('sys.platform', 'linux'), ('Python', '3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0]'), ('CUDA available', True), ('MUSA available', False), ('numpy_random_seed', 2147483648), ('GPU 0,1,2,3,4,5,6,7', 'NVIDIA A100-PCIE-40GB'), ('CUDA_HOME', '/usr/local/cuda'), ('NVCC', 'Cuda compilation tools, release 11.8, V11.8.89'), ('GCC', 'gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0'), ('PyTorch', '1.13.1+cu117'), ('PyTorch compiling details', 'PyTorch built with:\\n  - GCC 9.3\\n  - C++ Version: 201402\\n  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications\\n  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)\\n  - OpenMP 201511 (a.k.a. OpenMP 4.5)\\n  - LAPACK is enabled (usually provided by MKL)\\n  - NNPACK is enabled\\n  - CPU capability usage: AVX2\\n  - CUDA Runtime 11.7\\n  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86\\n  - CuDNN 8.5\\n  - Magma 2.6.1\\n  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \\n'), ('TorchVision', '0.14.1+cu117'), ('OpenCV', '4.10.0'), ('MMEngine', '0.10.5')])\n"
     ]
    }
   ],
   "source": [
    "# Check MMEngine installation\n",
    "from mmengine.utils.dl_utils import collect_env\n",
    "print(collect_env())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf5f9db-e5eb-41f3-a059-0e879627d208",
   "metadata": {},
   "source": [
    "# 二、使用MMAction2识别器执行推理\n",
    "MMAction2已经提供了高级API来进行推理和训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7187388d-df4a-4c9b-b669-17fe85fdddbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TSN-R50-1x1x3 作为基于 rgb 的动作识别器\n",
    "# checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8a96d70-4a37-453d-9d15-94f2dc66336d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: ../checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth\n"
     ]
    }
   ],
   "source": [
    "from mmaction.apis import inference_recognizer, init_recognizer\n",
    "from mmengine import Config\n",
    "\n",
    "\n",
    "# Choose to use a config and initialize the recognizer\n",
    "config = '../configs/recognition/tsn/tsn_imagenet-pretrained-r50_8xb32-1x1x3-100e_kinetics400-rgb.py'\n",
    "config = Config.fromfile(config)\n",
    "# Setup a checkpoint file to load\n",
    "checkpoint = '../checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth'\n",
    "# Initialize the recognizer\n",
    "model = init_recognizer(config, checkpoint, device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6854bf81-6595-4914-82d1-d8f42d73bcee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/17 02:56:03 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
      "10/17 02:56:03 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"HardDiskBackend\" is the alias of \"LocalBackend\" and the former will be deprecated in future.\n"
     ]
    }
   ],
   "source": [
    "# Use the recognizer to do inference\n",
    "from operator import itemgetter\n",
    "video = '../demo/demo.mp4'\n",
    "label = '../tools/data/kinetics/label_map_k400.txt'\n",
    "results = inference_recognizer(model, video)\n",
    "\n",
    "pred_scores = results.pred_score.tolist()\n",
    "score_tuples = tuple(zip(range(len(pred_scores)), pred_scores))\n",
    "score_sorted = sorted(score_tuples, key=itemgetter(1), reverse=True)\n",
    "top5_label = score_sorted[:5]\n",
    "\n",
    "labels = open(label).readlines()\n",
    "labels = [x.strip() for x in labels]\n",
    "results = [(labels[k[0]], k[1]) for k in top5_label]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "528f8f89-9e88-47d3-9b3d-24c9a4dcae10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top-5 labels with corresponding scores are:\n",
      "arm wrestling:  1.0\n",
      "rock scissors paper:  6.434453414527752e-09\n",
      "shaking hands:  2.758343997655288e-09\n",
      "clapping:  1.3451096902983295e-09\n",
      "massaging feet:  5.551171189388526e-10\n"
     ]
    }
   ],
   "source": [
    "print('The top-5 labels with corresponding scores are:')\n",
    "for result in results:\n",
    "    print(f'{result[0]}: ', result[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59ec3d6-ade1-4355-bbc1-f450cacede84",
   "metadata": {},
   "source": [
    "# 三、在自定义数据集上训练识别器\n",
    "要训练新的识别器，通常有三件事要做：\n",
    "\n",
    "    支持新数据集\n",
    "    修改配置\n",
    "    训练新的识别器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79563a50-d8d9-4e0b-968a-a1170fe53983",
   "metadata": {},
   "source": [
    "## 支持新数据集\n",
    "在本教程中，我们给出了一个将数据转换为现有数据集格式的示例。其他方法和更高级的用法可以在文档中找到\n",
    "首先，让我们下载一个从Kinetics-400获得的小数据集。我们选择了30个带有标签的视频作为训练数据集，10个带有标签视频作为测试数据集。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5544bbe-42e4-45f7-a923-ee50bff28137",
   "metadata": {},
   "source": [
    "## 修改配置\n",
    "下一步，我们需要修改训练的配置。为了加速这一过程，我们使用预训练的识别器对识别器进行微调\n",
    "\n",
    "TSN 模型中的帧采样策略 1x1x3 表示什么？\n",
    "\n",
    "在 TSN（Temporal Segment Networks） 模型中，帧采样策略通常以 N x L x T 的形式表示，其中：\n",
    "\n",
    "ampleFrames 定义输入帧的示例策略。示例策略定义为clip_len xframe_interval x num_clips。\n",
    "\n",
    "    clip_len=1意味着每个剪辑只包含1帧。\n",
    "    frame_interval=1意味着每帧都被选取。\n",
    "    num_clips=3意味着从视频中会抽取3个剪辑。\n",
    "密集策略演示：32x2x1可选择1、3、5、...、63帧。\n",
    "\n",
    "统一策略演示：1x1x8 将 100 帧分为 [1, 12]、[13, 24]、[25, 36]、[37, 48]、...、[85, 96]，并且可以选择 1、13 , 25, ..., 85 帧。及将视频分为8个片段，每frame_interval 1 帧采样一帧，最终每个片段得到clip_len帧。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e5b89ca8-639e-40ee-befe-b83852d6f5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config.fromfile('../configs/recognition/tsn/tsn_imagenet-pretrained-r50_8xb32-1x1x3-100e_kinetics400-rgb.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae9f189-ad52-44f7-bf52-25dbac37b808",
   "metadata": {},
   "source": [
    "给定一个在kinetics400完整数据集上训练TSN模型的配置，我们需要修改一些值，将其用于在kinetics400微小数据集上培训TSN。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f5e3646d-266d-4c04-9af3-e757ad173ae6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size: 2\n",
      "batch_size : 0, 0\n",
      "Config:\n",
      "ann_file_train = '/root/project/research/action/mmaction2/data/kinetics400_tiny/kinetics_tiny_train_video.txt'\n",
      "ann_file_val = '/root/project/research/action/mmaction2/data/kinetics400_tiny/kinetics_tiny_val_video.txt'\n",
      "auto_scale_lr = dict(base_batch_size=256, enable=False)\n",
      "data_root = '/root/project/research/action/mmaction2/data/kinetics400_tiny/train/'\n",
      "data_root_val = '/root/project/research/action/mmaction2/data/kinetics400_tiny/val/'\n",
      "dataset_type = 'VideoDataset'\n",
      "default_hooks = dict(\n",
      "    checkpoint=dict(\n",
      "        interval=3, max_keep_ckpts=3, save_best='auto', type='CheckpointHook'),\n",
      "    logger=dict(ignore_last=False, interval=20, type='LoggerHook'),\n",
      "    param_scheduler=dict(type='ParamSchedulerHook'),\n",
      "    runtime_info=dict(type='RuntimeInfoHook'),\n",
      "    sampler_seed=dict(type='DistSamplerSeedHook'),\n",
      "    sync_buffers=dict(type='SyncBuffersHook'),\n",
      "    timer=dict(type='IterTimerHook'))\n",
      "default_scope = 'mmaction'\n",
      "env_cfg = dict(\n",
      "    cudnn_benchmark=False,\n",
      "    dist_cfg=dict(backend='nccl'),\n",
      "    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))\n",
      "file_client_args = dict(io_backend='disk')\n",
      "load_from = '/root/project/research/action/mmaction2/checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth'\n",
      "log_level = 'INFO'\n",
      "log_processor = dict(by_epoch=True, type='LogProcessor', window_size=20)\n",
      "model = dict(\n",
      "    backbone=dict(\n",
      "        depth=50,\n",
      "        norm_eval=False,\n",
      "        pretrained='https://download.pytorch.org/models/resnet50-11ad3fa6.pth',\n",
      "        type='ResNet'),\n",
      "    cls_head=dict(\n",
      "        average_clips='prob',\n",
      "        consensus=dict(dim=1, type='AvgConsensus'),\n",
      "        dropout_ratio=0.4,\n",
      "        in_channels=2048,\n",
      "        init_std=0.01,\n",
      "        num_classes=2,\n",
      "        spatial_type='avg',\n",
      "        type='TSNHead'),\n",
      "    data_preprocessor=dict(\n",
      "        format_shape='NCHW',\n",
      "        mean=[\n",
      "            123.675,\n",
      "            116.28,\n",
      "            103.53,\n",
      "        ],\n",
      "        std=[\n",
      "            58.395,\n",
      "            57.12,\n",
      "            57.375,\n",
      "        ],\n",
      "        type='ActionDataPreprocessor'),\n",
      "    test_cfg=None,\n",
      "    train_cfg=None,\n",
      "    type='Recognizer2D')\n",
      "optim_wrapper = dict(\n",
      "    clip_grad=dict(max_norm=40, norm_type=2),\n",
      "    optimizer=dict(\n",
      "        lr=6.103515625e-07, momentum=0.9, type='SGD', weight_decay=0.0001))\n",
      "param_scheduler = [\n",
      "    dict(\n",
      "        begin=0,\n",
      "        by_epoch=True,\n",
      "        end=100,\n",
      "        gamma=0.1,\n",
      "        milestones=[\n",
      "            40,\n",
      "            80,\n",
      "        ],\n",
      "        type='MultiStepLR'),\n",
      "]\n",
      "resume = False\n",
      "test_cfg = dict(type='TestLoop')\n",
      "test_dataloader = dict(\n",
      "    batch_size=1,\n",
      "    dataset=dict(\n",
      "        ann_file=\n",
      "        '/root/project/research/action/mmaction2/data/kinetics400_tiny/kinetics_tiny_val_video.txt',\n",
      "        data_prefix=dict(\n",
      "            video=\n",
      "            '/root/project/research/action/mmaction2/data/kinetics400_tiny/val/'\n",
      "        ),\n",
      "        pipeline=[\n",
      "            dict(io_backend='disk', type='DecordInit'),\n",
      "            dict(\n",
      "                clip_len=1,\n",
      "                frame_interval=1,\n",
      "                num_clips=25,\n",
      "                test_mode=True,\n",
      "                type='SampleFrames'),\n",
      "            dict(type='DecordDecode'),\n",
      "            dict(scale=(\n",
      "                -1,\n",
      "                256,\n",
      "            ), type='Resize'),\n",
      "            dict(crop_size=224, type='TenCrop'),\n",
      "            dict(input_format='NCHW', type='FormatShape'),\n",
      "            dict(type='PackActionInputs'),\n",
      "        ],\n",
      "        test_mode=True,\n",
      "        type='VideoDataset'),\n",
      "    num_workers=2,\n",
      "    persistent_workers=True,\n",
      "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
      "test_evaluator = dict(type='AccMetric')\n",
      "test_pipeline = [\n",
      "    dict(io_backend='disk', type='DecordInit'),\n",
      "    dict(\n",
      "        clip_len=1,\n",
      "        frame_interval=1,\n",
      "        num_clips=25,\n",
      "        test_mode=True,\n",
      "        type='SampleFrames'),\n",
      "    dict(type='DecordDecode'),\n",
      "    dict(scale=(\n",
      "        -1,\n",
      "        256,\n",
      "    ), type='Resize'),\n",
      "    dict(crop_size=224, type='TenCrop'),\n",
      "    dict(input_format='NCHW', type='FormatShape'),\n",
      "    dict(type='PackActionInputs'),\n",
      "]\n",
      "train_cfg = dict(\n",
      "    max_epochs=20, type='EpochBasedTrainLoop', val_begin=1, val_interval=1)\n",
      "train_dataloader = dict(\n",
      "    batch_size=0,\n",
      "    dataset=dict(\n",
      "        ann_file=\n",
      "        '/root/project/research/action/mmaction2/data/kinetics400_tiny/kinetics_tiny_train_video.txt',\n",
      "        data_prefix=dict(\n",
      "            video=\n",
      "            '/root/project/research/action/mmaction2/data/kinetics400_tiny/train/'\n",
      "        ),\n",
      "        pipeline=[\n",
      "            dict(io_backend='disk', type='DecordInit'),\n",
      "            dict(\n",
      "                clip_len=1, frame_interval=1, num_clips=3,\n",
      "                type='SampleFrames'),\n",
      "            dict(type='DecordDecode'),\n",
      "            dict(scale=(\n",
      "                -1,\n",
      "                256,\n",
      "            ), type='Resize'),\n",
      "            dict(\n",
      "                input_size=224,\n",
      "                max_wh_scale_gap=1,\n",
      "                random_crop=False,\n",
      "                scales=(\n",
      "                    1,\n",
      "                    0.875,\n",
      "                    0.75,\n",
      "                    0.66,\n",
      "                ),\n",
      "                type='MultiScaleCrop'),\n",
      "            dict(keep_ratio=False, scale=(\n",
      "                224,\n",
      "                224,\n",
      "            ), type='Resize'),\n",
      "            dict(flip_ratio=0.5, type='Flip'),\n",
      "            dict(input_format='NCHW', type='FormatShape'),\n",
      "            dict(type='PackActionInputs'),\n",
      "        ],\n",
      "        type='VideoDataset'),\n",
      "    num_workers=2,\n",
      "    persistent_workers=True,\n",
      "    sampler=dict(shuffle=True, type='DefaultSampler'))\n",
      "train_pipeline = [\n",
      "    dict(io_backend='disk', type='DecordInit'),\n",
      "    dict(clip_len=1, frame_interval=1, num_clips=3, type='SampleFrames'),\n",
      "    dict(type='DecordDecode'),\n",
      "    dict(scale=(\n",
      "        -1,\n",
      "        256,\n",
      "    ), type='Resize'),\n",
      "    dict(\n",
      "        input_size=224,\n",
      "        max_wh_scale_gap=1,\n",
      "        random_crop=False,\n",
      "        scales=(\n",
      "            1,\n",
      "            0.875,\n",
      "            0.75,\n",
      "            0.66,\n",
      "        ),\n",
      "        type='MultiScaleCrop'),\n",
      "    dict(keep_ratio=False, scale=(\n",
      "        224,\n",
      "        224,\n",
      "    ), type='Resize'),\n",
      "    dict(flip_ratio=0.5, type='Flip'),\n",
      "    dict(input_format='NCHW', type='FormatShape'),\n",
      "    dict(type='PackActionInputs'),\n",
      "]\n",
      "val_cfg = dict(type='ValLoop')\n",
      "val_dataloader = dict(\n",
      "    batch_size=0,\n",
      "    dataset=dict(\n",
      "        ann_file=\n",
      "        '/root/project/research/action/mmaction2/data/kinetics400_tiny/kinetics_tiny_val_video.txt',\n",
      "        data_prefix=dict(\n",
      "            video=\n",
      "            '/root/project/research/action/mmaction2/data/kinetics400_tiny/val/'\n",
      "        ),\n",
      "        pipeline=[\n",
      "            dict(io_backend='disk', type='DecordInit'),\n",
      "            dict(\n",
      "                clip_len=1,\n",
      "                frame_interval=1,\n",
      "                num_clips=3,\n",
      "                test_mode=True,\n",
      "                type='SampleFrames'),\n",
      "            dict(type='DecordDecode'),\n",
      "            dict(scale=(\n",
      "                -1,\n",
      "                256,\n",
      "            ), type='Resize'),\n",
      "            dict(crop_size=224, type='CenterCrop'),\n",
      "            dict(input_format='NCHW', type='FormatShape'),\n",
      "            dict(type='PackActionInputs'),\n",
      "        ],\n",
      "        test_mode=True,\n",
      "        type='VideoDataset'),\n",
      "    num_workers=2,\n",
      "    persistent_workers=True,\n",
      "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
      "val_evaluator = dict(type='AccMetric')\n",
      "val_pipeline = [\n",
      "    dict(io_backend='disk', type='DecordInit'),\n",
      "    dict(\n",
      "        clip_len=1,\n",
      "        frame_interval=1,\n",
      "        num_clips=3,\n",
      "        test_mode=True,\n",
      "        type='SampleFrames'),\n",
      "    dict(type='DecordDecode'),\n",
      "    dict(scale=(\n",
      "        -1,\n",
      "        256,\n",
      "    ), type='Resize'),\n",
      "    dict(crop_size=224, type='CenterCrop'),\n",
      "    dict(input_format='NCHW', type='FormatShape'),\n",
      "    dict(type='PackActionInputs'),\n",
      "]\n",
      "vis_backends = [\n",
      "    dict(type='LocalVisBackend'),\n",
      "]\n",
      "visualizer = dict(\n",
      "    type='ActionVisualizer', vis_backends=[\n",
      "        dict(type='LocalVisBackend'),\n",
      "    ])\n",
      "work_dir = './tutorial_exps'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mmengine.runner import set_random_seed\n",
    "\n",
    "# Modify dataset type and path\n",
    "cfg.data_root = '/root/project/research/action/mmaction2/data/kinetics400_tiny/train/'\n",
    "cfg.data_root_val = '/root/project/research/action/mmaction2/data/kinetics400_tiny/val/'\n",
    "cfg.ann_file_train = '/root/project/research/action/mmaction2/data/kinetics400_tiny/kinetics_tiny_train_video.txt'\n",
    "cfg.ann_file_val = '/root/project/research/action/mmaction2/data/kinetics400_tiny/kinetics_tiny_val_video.txt'\n",
    "\n",
    "\n",
    "cfg.test_dataloader.dataset.ann_file = '/root/project/research/action/mmaction2/data/kinetics400_tiny/kinetics_tiny_val_video.txt'\n",
    "cfg.test_dataloader.dataset.data_prefix.video = '/root/project/research/action/mmaction2/data/kinetics400_tiny/val/'\n",
    "\n",
    "cfg.train_dataloader.dataset.ann_file = '/root/project/research/action/mmaction2/data/kinetics400_tiny/kinetics_tiny_train_video.txt'\n",
    "cfg.train_dataloader.dataset.data_prefix.video = '/root/project/research/action/mmaction2/data/kinetics400_tiny/train/'\n",
    "\n",
    "cfg.val_dataloader.dataset.ann_file = '/root/project/research/action/mmaction2/data/kinetics400_tiny/kinetics_tiny_val_video.txt'\n",
    "cfg.val_dataloader.dataset.data_prefix.video  = '/root/project/research/action/mmaction2/data/kinetics400_tiny/val/'\n",
    "\n",
    "\n",
    "# Modify num classes of the model in cls_head\n",
    "cfg.model.cls_head.num_classes = 2\n",
    "# We can use the pre-trained TSN model\n",
    "cfg.load_from = '/root/project/research/action/mmaction2/checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth'\n",
    "\n",
    "# Set up working dir to save files and logs.\n",
    "cfg.work_dir = './tutorial_exps'\n",
    "\n",
    "# The original learning rate (LR) is set for 8-GPU training.\n",
    "# We divide it by 8 since we only use one GPU.\n",
    "# 原本的配置是为 8-GPU 的训练设置的。通常，多GPU训练时会采用较大的 batch size（每个 GPU 上有一定的 batch size，总的 batch size 为多个 GPU 的总和）。\n",
    "print(f\"batch size: {cfg.train_dataloader.batch_size}\")\n",
    "cfg.train_dataloader.batch_size = cfg.train_dataloader.batch_size // 16\n",
    "cfg.val_dataloader.batch_size = cfg.val_dataloader.batch_size // 16\n",
    "print(f\"batch_size : {cfg.train_dataloader.batch_size}, {cfg.val_dataloader.batch_size}\")\n",
    "# 学习率（LR） 也需要根据 GPU 数量 和 batch size 进行调整。\n",
    "# 原始学习率是为 8-GPU 训练设计的。常见的经验法则是：学习率与总的 batch size 成比例。当 batch size 减少时，学习率也应该相应减少。\n",
    "# 这里，学习率先除以 8，是为了适应从 8 个 GPU 到 1 个 GPU 的转换，再除以 16，是因为 batch size 也相应缩小了 16 倍。\n",
    "cfg.optim_wrapper.optimizer.lr = cfg.optim_wrapper.optimizer.lr / 8 / 16\n",
    "# 小数据集收敛速度更快，过多的 epochs 可能导致 过拟合。\n",
    "cfg.train_cfg.max_epochs = 10\n",
    "\n",
    "# num_workers 指定了数据加载过程中用于数据预处理的工作线程数。更多的工作线程可以加速数据加载，尤其是对于大数据集。\n",
    "# 但是对于 小数据集（如 kinetics400_tiny），较少的工作线程已经足够完成数据加载，且在单GPU训练时，减少 num_workers 也有助于节省系统资源。\n",
    "cfg.train_dataloader.num_workers = 2\n",
    "cfg.val_dataloader.num_workers = 2\n",
    "cfg.test_dataloader.num_workers = 2\n",
    "\n",
    "# We can initialize the logger for training and have a look at the final config used for training\n",
    "# 我们可以初始化记录器进行训练，并查看用于训练的最终配置\n",
    "print(f'Config:\\n{cfg.pretty_text}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c209e39-e6a5-4078-af47-982c61dbece4",
   "metadata": {},
   "source": [
    "# 四、训练新的识别器\n",
    "最后，让我们初始化数据集和识别器，然后训练一个新的识别器！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad1f4ddd-32d5-451b-9658-9b52817c0c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/17 08:38:29 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "------------------------------------------------------------\n",
      "System environment:\n",
      "    sys.platform: linux\n",
      "    Python: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0]\n",
      "    CUDA available: True\n",
      "    MUSA available: False\n",
      "    numpy_random_seed: 1605309348\n",
      "    GPU 0,1,2,3,4,5,6,7: NVIDIA A100-PCIE-40GB\n",
      "    CUDA_HOME: /usr/local/cuda\n",
      "    NVCC: Cuda compilation tools, release 11.8, V11.8.89\n",
      "    GCC: gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\n",
      "    PyTorch: 1.13.1+cu117\n",
      "    PyTorch compiling details: PyTorch built with:\n",
      "  - GCC 9.3\n",
      "  - C++ Version: 201402\n",
      "  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)\n",
      "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "  - LAPACK is enabled (usually provided by MKL)\n",
      "  - NNPACK is enabled\n",
      "  - CPU capability usage: AVX2\n",
      "  - CUDA Runtime 11.7\n",
      "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86\n",
      "  - CuDNN 8.5\n",
      "  - Magma 2.6.1\n",
      "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
      "\n",
      "    TorchVision: 0.14.1+cu117\n",
      "    OpenCV: 4.10.0\n",
      "    MMEngine: 0.10.5\n",
      "\n",
      "Runtime environment:\n",
      "    cudnn_benchmark: False\n",
      "    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}\n",
      "    dist_cfg: {'backend': 'nccl'}\n",
      "    seed: 1605309348\n",
      "    Distributed launcher: none\n",
      "    Distributed training: False\n",
      "    GPU number: 1\n",
      "------------------------------------------------------------\n",
      "\n",
      "10/17 08:38:29 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Config:\n",
      "ann_file_train = '/root/project/research/action/mmaction2/data/kinetics400_tiny/kinetics_tiny_train_video.txt'\n",
      "ann_file_val = '/root/project/research/action/mmaction2/data/kinetics400_tiny/kinetics_tiny_val_video.txt'\n",
      "auto_scale_lr = dict(base_batch_size=256, enable=False)\n",
      "data_root = '/root/project/research/action/mmaction2/data/kinetics400_tiny/train/'\n",
      "data_root_val = '/root/project/research/action/mmaction2/data/kinetics400_tiny/val/'\n",
      "dataset_type = 'VideoDataset'\n",
      "default_hooks = dict(\n",
      "    checkpoint=dict(\n",
      "        interval=3, max_keep_ckpts=3, save_best='auto', type='CheckpointHook'),\n",
      "    logger=dict(ignore_last=False, interval=20, type='LoggerHook'),\n",
      "    param_scheduler=dict(type='ParamSchedulerHook'),\n",
      "    runtime_info=dict(type='RuntimeInfoHook'),\n",
      "    sampler_seed=dict(type='DistSamplerSeedHook'),\n",
      "    sync_buffers=dict(type='SyncBuffersHook'),\n",
      "    timer=dict(type='IterTimerHook'))\n",
      "default_scope = 'mmaction'\n",
      "env_cfg = dict(\n",
      "    cudnn_benchmark=False,\n",
      "    dist_cfg=dict(backend='nccl'),\n",
      "    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))\n",
      "file_client_args = dict(io_backend='disk')\n",
      "load_from = '/root/project/research/action/mmaction2/checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth'\n",
      "log_level = 'INFO'\n",
      "log_processor = dict(by_epoch=True, type='LogProcessor', window_size=20)\n",
      "model = dict(\n",
      "    backbone=dict(\n",
      "        depth=50,\n",
      "        norm_eval=False,\n",
      "        pretrained='https://download.pytorch.org/models/resnet50-11ad3fa6.pth',\n",
      "        type='ResNet'),\n",
      "    cls_head=dict(\n",
      "        average_clips='prob',\n",
      "        consensus=dict(dim=1, type='AvgConsensus'),\n",
      "        dropout_ratio=0.4,\n",
      "        in_channels=2048,\n",
      "        init_std=0.01,\n",
      "        num_classes=2,\n",
      "        spatial_type='avg',\n",
      "        type='TSNHead'),\n",
      "    data_preprocessor=dict(\n",
      "        format_shape='NCHW',\n",
      "        mean=[\n",
      "            123.675,\n",
      "            116.28,\n",
      "            103.53,\n",
      "        ],\n",
      "        std=[\n",
      "            58.395,\n",
      "            57.12,\n",
      "            57.375,\n",
      "        ],\n",
      "        type='ActionDataPreprocessor'),\n",
      "    test_cfg=None,\n",
      "    train_cfg=None,\n",
      "    type='Recognizer2D')\n",
      "optim_wrapper = dict(\n",
      "    clip_grad=dict(max_norm=40, norm_type=2),\n",
      "    optimizer=dict(\n",
      "        lr=7.8125e-05, momentum=0.9, type='SGD', weight_decay=0.0001))\n",
      "param_scheduler = [\n",
      "    dict(\n",
      "        begin=0,\n",
      "        by_epoch=True,\n",
      "        end=100,\n",
      "        gamma=0.1,\n",
      "        milestones=[\n",
      "            40,\n",
      "            80,\n",
      "        ],\n",
      "        type='MultiStepLR'),\n",
      "]\n",
      "resume = False\n",
      "test_cfg = dict(type='TestLoop')\n",
      "test_dataloader = dict(\n",
      "    batch_size=1,\n",
      "    dataset=dict(\n",
      "        ann_file=\n",
      "        '/root/project/research/action/mmaction2/data/kinetics400_tiny/kinetics_tiny_val_video.txt',\n",
      "        data_prefix=dict(\n",
      "            video=\n",
      "            '/root/project/research/action/mmaction2/data/kinetics400_tiny/val/'\n",
      "        ),\n",
      "        pipeline=[\n",
      "            dict(io_backend='disk', type='DecordInit'),\n",
      "            dict(\n",
      "                clip_len=1,\n",
      "                frame_interval=1,\n",
      "                num_clips=25,\n",
      "                test_mode=True,\n",
      "                type='SampleFrames'),\n",
      "            dict(type='DecordDecode'),\n",
      "            dict(scale=(\n",
      "                -1,\n",
      "                256,\n",
      "            ), type='Resize'),\n",
      "            dict(crop_size=224, type='TenCrop'),\n",
      "            dict(input_format='NCHW', type='FormatShape'),\n",
      "            dict(type='PackActionInputs'),\n",
      "        ],\n",
      "        test_mode=True,\n",
      "        type='VideoDataset'),\n",
      "    num_workers=2,\n",
      "    persistent_workers=True,\n",
      "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
      "test_evaluator = dict(type='AccMetric')\n",
      "test_pipeline = [\n",
      "    dict(io_backend='disk', type='DecordInit'),\n",
      "    dict(\n",
      "        clip_len=1,\n",
      "        frame_interval=1,\n",
      "        num_clips=25,\n",
      "        test_mode=True,\n",
      "        type='SampleFrames'),\n",
      "    dict(type='DecordDecode'),\n",
      "    dict(scale=(\n",
      "        -1,\n",
      "        256,\n",
      "    ), type='Resize'),\n",
      "    dict(crop_size=224, type='TenCrop'),\n",
      "    dict(input_format='NCHW', type='FormatShape'),\n",
      "    dict(type='PackActionInputs'),\n",
      "]\n",
      "train_cfg = dict(\n",
      "    max_epochs=10, type='EpochBasedTrainLoop', val_begin=1, val_interval=1)\n",
      "train_dataloader = dict(\n",
      "    batch_size=2,\n",
      "    dataset=dict(\n",
      "        ann_file=\n",
      "        '/root/project/research/action/mmaction2/data/kinetics400_tiny/kinetics_tiny_train_video.txt',\n",
      "        data_prefix=dict(\n",
      "            video=\n",
      "            '/root/project/research/action/mmaction2/data/kinetics400_tiny/train/'\n",
      "        ),\n",
      "        pipeline=[\n",
      "            dict(io_backend='disk', type='DecordInit'),\n",
      "            dict(\n",
      "                clip_len=1, frame_interval=1, num_clips=3,\n",
      "                type='SampleFrames'),\n",
      "            dict(type='DecordDecode'),\n",
      "            dict(scale=(\n",
      "                -1,\n",
      "                256,\n",
      "            ), type='Resize'),\n",
      "            dict(\n",
      "                input_size=224,\n",
      "                max_wh_scale_gap=1,\n",
      "                random_crop=False,\n",
      "                scales=(\n",
      "                    1,\n",
      "                    0.875,\n",
      "                    0.75,\n",
      "                    0.66,\n",
      "                ),\n",
      "                type='MultiScaleCrop'),\n",
      "            dict(keep_ratio=False, scale=(\n",
      "                224,\n",
      "                224,\n",
      "            ), type='Resize'),\n",
      "            dict(flip_ratio=0.5, type='Flip'),\n",
      "            dict(input_format='NCHW', type='FormatShape'),\n",
      "            dict(type='PackActionInputs'),\n",
      "        ],\n",
      "        type='VideoDataset'),\n",
      "    num_workers=2,\n",
      "    persistent_workers=True,\n",
      "    sampler=dict(shuffle=True, type='DefaultSampler'))\n",
      "train_pipeline = [\n",
      "    dict(io_backend='disk', type='DecordInit'),\n",
      "    dict(clip_len=1, frame_interval=1, num_clips=3, type='SampleFrames'),\n",
      "    dict(type='DecordDecode'),\n",
      "    dict(scale=(\n",
      "        -1,\n",
      "        256,\n",
      "    ), type='Resize'),\n",
      "    dict(\n",
      "        input_size=224,\n",
      "        max_wh_scale_gap=1,\n",
      "        random_crop=False,\n",
      "        scales=(\n",
      "            1,\n",
      "            0.875,\n",
      "            0.75,\n",
      "            0.66,\n",
      "        ),\n",
      "        type='MultiScaleCrop'),\n",
      "    dict(keep_ratio=False, scale=(\n",
      "        224,\n",
      "        224,\n",
      "    ), type='Resize'),\n",
      "    dict(flip_ratio=0.5, type='Flip'),\n",
      "    dict(input_format='NCHW', type='FormatShape'),\n",
      "    dict(type='PackActionInputs'),\n",
      "]\n",
      "val_cfg = dict(type='ValLoop')\n",
      "val_dataloader = dict(\n",
      "    batch_size=2,\n",
      "    dataset=dict(\n",
      "        ann_file=\n",
      "        '/root/project/research/action/mmaction2/data/kinetics400_tiny/kinetics_tiny_val_video.txt',\n",
      "        data_prefix=dict(\n",
      "            video=\n",
      "            '/root/project/research/action/mmaction2/data/kinetics400_tiny/val/'\n",
      "        ),\n",
      "        pipeline=[\n",
      "            dict(io_backend='disk', type='DecordInit'),\n",
      "            dict(\n",
      "                clip_len=1,\n",
      "                frame_interval=1,\n",
      "                num_clips=3,\n",
      "                test_mode=True,\n",
      "                type='SampleFrames'),\n",
      "            dict(type='DecordDecode'),\n",
      "            dict(scale=(\n",
      "                -1,\n",
      "                256,\n",
      "            ), type='Resize'),\n",
      "            dict(crop_size=224, type='CenterCrop'),\n",
      "            dict(input_format='NCHW', type='FormatShape'),\n",
      "            dict(type='PackActionInputs'),\n",
      "        ],\n",
      "        test_mode=True,\n",
      "        type='VideoDataset'),\n",
      "    num_workers=2,\n",
      "    persistent_workers=True,\n",
      "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
      "val_evaluator = dict(type='AccMetric')\n",
      "val_pipeline = [\n",
      "    dict(io_backend='disk', type='DecordInit'),\n",
      "    dict(\n",
      "        clip_len=1,\n",
      "        frame_interval=1,\n",
      "        num_clips=3,\n",
      "        test_mode=True,\n",
      "        type='SampleFrames'),\n",
      "    dict(type='DecordDecode'),\n",
      "    dict(scale=(\n",
      "        -1,\n",
      "        256,\n",
      "    ), type='Resize'),\n",
      "    dict(crop_size=224, type='CenterCrop'),\n",
      "    dict(input_format='NCHW', type='FormatShape'),\n",
      "    dict(type='PackActionInputs'),\n",
      "]\n",
      "vis_backends = [\n",
      "    dict(type='LocalVisBackend'),\n",
      "]\n",
      "visualizer = dict(\n",
      "    type='ActionVisualizer', vis_backends=[\n",
      "        dict(type='LocalVisBackend'),\n",
      "    ])\n",
      "work_dir = './tutorial_exps'\n",
      "\n",
      "10/17 08:38:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
      "10/17 08:38:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "before_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) DistSamplerSeedHook                \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) SyncBuffersHook                    \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) SyncBuffersHook                    \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_test_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_run:\n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "Loads checkpoint by http backend from path: https://download.pytorch.org/models/resnet50-11ad3fa6.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/17 08:49:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - These parameters in pretrained checkpoint are not loaded: {'fc.weight', 'fc.bias'}\n",
      "Loads checkpoint by local backend from path: /root/project/research/action/mmaction2/checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth\n",
      "The model and loaded state dict do not match exactly\n",
      "\n",
      "size mismatch for cls_head.fc_cls.weight: copying a param with shape torch.Size([400, 2048]) from checkpoint, the shape in current model is torch.Size([2, 2048]).\n",
      "size mismatch for cls_head.fc_cls.bias: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([2]).\n",
      "10/17 08:49:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Load checkpoint from /root/project/research/action/mmaction2/checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth\n",
      "10/17 08:49:36 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
      "10/17 08:49:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Checkpoints will be saved to /root/project/research/action/mmaction2/demo/tutorial_exps.\n",
      "10/17 08:49:39 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: tsn_imagenet-pretrained-r50_8xb32-1x1x3-100e_kinetics400-rgb_20241017_083829\n",
      "10/17 08:49:39 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [1][15/15]  lr: 7.8125e-05  eta: 0:00:27  time: 0.2064  data_time: 0.0587  memory: 2918  grad_norm: 12.7283  loss: 0.7042  top1_acc: 0.0000  top5_acc: 1.0000  loss_cls: 0.7042\n",
      "10/17 08:49:41 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][5/5]    acc/top1: 0.3000  acc/top5: 1.0000  acc/mean1: 0.3000  data_time: 0.2046  time: 0.2340\n",
      "10/17 08:49:41 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - The best checkpoint with 0.3000 acc/top1 at 1 epoch is saved to best_acc_top1_epoch_1.pth.\n",
      "10/17 08:49:43 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: tsn_imagenet-pretrained-r50_8xb32-1x1x3-100e_kinetics400-rgb_20241017_083829\n",
      "10/17 08:49:43 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [2][15/15]  lr: 7.8125e-05  eta: 0:00:21  time: 0.1514  data_time: 0.0665  memory: 1081  grad_norm: 11.4739  loss: 0.6820  top1_acc: 0.5000  top5_acc: 1.0000  loss_cls: 0.6820\n",
      "10/17 08:49:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][5/5]    acc/top1: 0.5000  acc/top5: 1.0000  acc/mean1: 0.5000  data_time: 0.1777  time: 0.2068\n",
      "10/17 08:49:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - The previous best checkpoint /root/project/research/action/mmaction2/demo/tutorial_exps/best_acc_top1_epoch_1.pth is removed\n",
      "10/17 08:49:45 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - The best checkpoint with 0.5000 acc/top1 at 2 epoch is saved to best_acc_top1_epoch_2.pth.\n",
      "10/17 08:49:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: tsn_imagenet-pretrained-r50_8xb32-1x1x3-100e_kinetics400-rgb_20241017_083829\n",
      "10/17 08:49:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [3][15/15]  lr: 7.8125e-05  eta: 0:00:17  time: 0.1490  data_time: 0.0602  memory: 1081  grad_norm: 12.4471  loss: 0.6798  top1_acc: 0.5000  top5_acc: 1.0000  loss_cls: 0.6798\n",
      "10/17 08:49:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 3 epochs\n",
      "10/17 08:49:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [3][5/5]    acc/top1: 0.5000  acc/top5: 1.0000  acc/mean1: 0.5000  data_time: 0.1733  time: 0.2025\n",
      "10/17 08:49:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: tsn_imagenet-pretrained-r50_8xb32-1x1x3-100e_kinetics400-rgb_20241017_083829\n",
      "10/17 08:49:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [4][15/15]  lr: 7.8125e-05  eta: 0:00:14  time: 0.1467  data_time: 0.0527  memory: 1081  grad_norm: 12.7430  loss: 0.6657  top1_acc: 0.0000  top5_acc: 1.0000  loss_cls: 0.6657\n",
      "10/17 08:49:52 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [4][5/5]    acc/top1: 0.7000  acc/top5: 1.0000  acc/mean1: 0.7000  data_time: 0.1811  time: 0.2152\n",
      "10/17 08:49:52 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - The previous best checkpoint /root/project/research/action/mmaction2/demo/tutorial_exps/best_acc_top1_epoch_2.pth is removed\n",
      "10/17 08:49:52 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - The best checkpoint with 0.7000 acc/top1 at 4 epoch is saved to best_acc_top1_epoch_4.pth.\n",
      "10/17 08:49:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: tsn_imagenet-pretrained-r50_8xb32-1x1x3-100e_kinetics400-rgb_20241017_083829\n",
      "10/17 08:49:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [5][15/15]  lr: 7.8125e-05  eta: 0:00:11  time: 0.1285  data_time: 0.0413  memory: 1081  grad_norm: 12.8335  loss: 0.6879  top1_acc: 0.0000  top5_acc: 1.0000  loss_cls: 0.6879\n",
      "10/17 08:49:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [5][5/5]    acc/top1: 0.6000  acc/top5: 1.0000  acc/mean1: 0.6000  data_time: 0.1794  time: 0.2137\n",
      "10/17 08:49:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: tsn_imagenet-pretrained-r50_8xb32-1x1x3-100e_kinetics400-rgb_20241017_083829\n",
      "10/17 08:49:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [6][15/15]  lr: 7.8125e-05  eta: 0:00:09  time: 0.1307  data_time: 0.0455  memory: 1081  grad_norm: 12.6256  loss: 0.6803  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.6803\n",
      "10/17 08:49:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 6 epochs\n",
      "10/17 08:50:00 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [6][5/5]    acc/top1: 0.6000  acc/top5: 1.0000  acc/mean1: 0.6000  data_time: 0.1700  time: 0.2041\n",
      "10/17 08:50:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: tsn_imagenet-pretrained-r50_8xb32-1x1x3-100e_kinetics400-rgb_20241017_083829\n",
      "10/17 08:50:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [7][15/15]  lr: 7.8125e-05  eta: 0:00:06  time: 0.1438  data_time: 0.0490  memory: 1081  grad_norm: 11.3501  loss: 0.6388  top1_acc: 0.5000  top5_acc: 1.0000  loss_cls: 0.6388\n",
      "10/17 08:50:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [7][5/5]    acc/top1: 0.9000  acc/top5: 1.0000  acc/mean1: 0.9000  data_time: 0.1774  time: 0.2062\n",
      "10/17 08:50:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - The previous best checkpoint /root/project/research/action/mmaction2/demo/tutorial_exps/best_acc_top1_epoch_4.pth is removed\n",
      "10/17 08:50:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - The best checkpoint with 0.9000 acc/top1 at 7 epoch is saved to best_acc_top1_epoch_7.pth.\n",
      "10/17 08:50:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: tsn_imagenet-pretrained-r50_8xb32-1x1x3-100e_kinetics400-rgb_20241017_083829\n",
      "10/17 08:50:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [8][15/15]  lr: 7.8125e-05  eta: 0:00:04  time: 0.1389  data_time: 0.0466  memory: 1081  grad_norm: 11.1063  loss: 0.6054  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.6054\n",
      "10/17 08:50:08 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [8][5/5]    acc/top1: 0.8000  acc/top5: 1.0000  acc/mean1: 0.8000  data_time: 0.1943  time: 0.2194\n",
      "10/17 08:50:10 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: tsn_imagenet-pretrained-r50_8xb32-1x1x3-100e_kinetics400-rgb_20241017_083829\n",
      "10/17 08:50:10 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [9][15/15]  lr: 7.8125e-05  eta: 0:00:02  time: 0.1305  data_time: 0.0392  memory: 1081  grad_norm: 10.6130  loss: 0.5601  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.5601\n",
      "10/17 08:50:10 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 9 epochs\n",
      "10/17 08:50:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [9][5/5]    acc/top1: 0.9000  acc/top5: 1.0000  acc/mean1: 0.9000  data_time: 0.1928  time: 0.2237\n",
      "10/17 08:50:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: tsn_imagenet-pretrained-r50_8xb32-1x1x3-100e_kinetics400-rgb_20241017_083829\n",
      "10/17 08:50:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [10][15/15]  lr: 7.8125e-05  eta: 0:00:00  time: 0.1457  data_time: 0.0549  memory: 1081  grad_norm: 11.1106  loss: 0.5652  top1_acc: 1.0000  top5_acc: 1.0000  loss_cls: 0.5652\n",
      "10/17 08:50:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 10 epochs\n",
      "10/17 08:50:16 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [10][5/5]    acc/top1: 0.9000  acc/top5: 1.0000  acc/mean1: 0.9000  data_time: 0.2028  time: 0.2314\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Recognizer2D(\n",
       "  (data_preprocessor): ActionDataPreprocessor()\n",
       "  (backbone): ResNet(\n",
       "    (conv1): ConvModule(\n",
       "      (conv): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activate): ReLU(inplace=True)\n",
       "    )\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): ConvModule(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvModule(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3): ConvModule(\n",
       "          (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): ConvModule(\n",
       "          (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): ConvModule(\n",
       "          (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvModule(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3): ConvModule(\n",
       "          (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): ConvModule(\n",
       "          (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvModule(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3): ConvModule(\n",
       "          (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): ConvModule(\n",
       "          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvModule(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3): ConvModule(\n",
       "          (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): ConvModule(\n",
       "          (conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): ConvModule(\n",
       "          (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvModule(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3): ConvModule(\n",
       "          (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): ConvModule(\n",
       "          (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvModule(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3): ConvModule(\n",
       "          (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): ConvModule(\n",
       "          (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvModule(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3): ConvModule(\n",
       "          (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): ConvModule(\n",
       "          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvModule(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3): ConvModule(\n",
       "          (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): ConvModule(\n",
       "          (conv): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): ConvModule(\n",
       "          (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvModule(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3): ConvModule(\n",
       "          (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): ConvModule(\n",
       "          (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvModule(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3): ConvModule(\n",
       "          (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): ConvModule(\n",
       "          (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvModule(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3): ConvModule(\n",
       "          (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): ConvModule(\n",
       "          (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvModule(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3): ConvModule(\n",
       "          (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): ConvModule(\n",
       "          (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvModule(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3): ConvModule(\n",
       "          (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): ConvModule(\n",
       "          (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvModule(\n",
       "          (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3): ConvModule(\n",
       "          (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): ConvModule(\n",
       "          (conv): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): ConvModule(\n",
       "          (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvModule(\n",
       "          (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3): ConvModule(\n",
       "          (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): ConvModule(\n",
       "          (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvModule(\n",
       "          (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3): ConvModule(\n",
       "          (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  init_cfg=[{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'layer': 'BatchNorm2d', 'val': 1.0}]\n",
       "  (cls_head): TSNHead(\n",
       "    (loss_cls): CrossEntropyLoss()\n",
       "    (consensus): AvgConsensus()\n",
       "    (avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (dropout): Dropout(p=0.4, inplace=False)\n",
       "    (fc_cls): Linear(in_features=2048, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os.path as osp\n",
    "import mmengine\n",
    "from mmengine.runner import Runner\n",
    "\n",
    "# Create work_dir\n",
    "mmengine.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
    "\n",
    "# build the runner from config\n",
    "runner = Runner.from_cfg(cfg)\n",
    "\n",
    "# start training\n",
    "runner.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c422c73c-7ae2-4518-a68d-b0719fb5a97b",
   "metadata": {},
   "source": [
    "# 五、了解日志\n",
    "从日志中，我们可以对训练过程有一个基本的了解，并知道识别器的训练情况。\n",
    "\n",
    "首先，加载在ImageNet上预训练的ResNet-50骨干网，这是一种常见的做法，因为从头开始训练成本更高。日志显示，除了fc.bias和fc.weight之外，ResNet-50骨干网的所有权重都已加载。\n",
    "\n",
    "其次，由于我们使用的数据集很小，我们加载了一个TSN模型并对其进行微调以进行动作识别。原始TSN是在原始Kinetics-400数据集上训练的，该数据集包含400个类，但Kinetics-400Tiny数据集只有2个类。因此，用于分类的预训练TSN的最后一个FC层具有不同的权重形状，因此未被使用。\n",
    "\n",
    "第三，训练后，识别器将按默认评估进行评估。结果表明，该识别器在val数据集上实现了100%的top1准确率和100%的top5准确率，\n",
    "\n",
    "不错！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c6445c-7984-4ba0-9dfe-1325c902b522",
   "metadata": {},
   "source": [
    "# 六、测试训练好的识别器\n",
    "在对识别器进行微调之后，让我们检查一下预测结果！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "536aa966-1328-4346-8e31-58162b700baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/17 08:53:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(test) [10/10]    acc/top1: 0.9000  acc/top5: 1.0000  acc/mean1: 0.9000  data_time: 0.0673  time: 0.2694\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc/top1': 0.9, 'acc/top5': 1.0, 'acc/mean1': 0.9}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runner.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3713967e-cb30-41e4-871c-d2bfbfb0b075",
   "metadata": {},
   "source": [
    "# 七、模型结构\n",
    "\n",
    "## TSN模型\n",
    "该模型被定义为一个2D动作识别器，主要用于处理视频数据并识别其中的动作。\n",
    "\n",
    "结构通过结合ResNet50作为特征提取器和TSNHead作为分类器，能够有效处理视频数据并识别多个动作类别。\n",
    "\n",
    "### backbone\n",
    "backbone部分是模型的主干网络，负责提取视频帧中的特征。 选择resnet50\n",
    "\n",
    "### cls_head\n",
    "cls_head部分是模型的分类头，负责将提取的特征映射到动作类别。\n",
    "\n",
    "type='TSNHead'：指定分类头的类型为TSNHead，适用于TSN模型的分类任务。\n",
    "\n",
    "    平均池化层\n",
    "    共识层\n",
    "    Dropout 层\n",
    "    线性分类层 fc_cls\n",
    "consensus=dict(type='AvgConsensus', dim=1)：定义共识函数为平均共识，沿着维度1进行操作，结合多个帧的特征以获得最终的分类结果。\n",
    "\n",
    "    AvgConsensus 是一个用于平均共识的模块，通常在处理视频分类任务时用于融合不同时间段的特征。\n",
    "\n",
    "    理解参考：https://blog.csdn.net/irving512/article/details/107501281\n",
    "    \n",
    "    输入: [N, num_segs, in_channels, 1, 1]， 输入的特征图尺寸其实是 N * num_segs，即包括了 batch size 以及一个clip中的T帧图片。在1x1x3中num_segs==1\n",
    "    每个样本的 num_segs 个时间段都有其对应的特征表示。 AvgConsensus 会计算时间维度上的平均值。\n",
    "    输出: [N, 1, in_channels, 1, 1]。这使得后续的分类头（如 TSNHead）可以更方便地处理这些融合后的特征。\n",
    "    \n",
    "    经过共识操作后，得到的特征只保留了一次样本的平均表示，去掉了 num_segs 维度。这个特征是对整个视频在所有时间段上进行融合的结果。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ba6e8c-2ede-487b-9792-6ae487773d4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
